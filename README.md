## 1. Настройка окружения

Клонируйте репозиторий:

```bash
git clone [адрес_вашего_репозитория]
cd [название_папки_проекта]
```

Создайте и активируйте виртуальное окружение:

```bash
python -m venv venv
.\venv\Scripts\activate
```

Установите все зависимости:

```bash
pip install -r requirements.txt
```

---

## 2. Проверка метрик на тестовой выборке

Эта команда запустит лучшую обученную модель (`yolov11m-hpo`) на тестовом наборе данных и выведет в консоль таблицу с финальными метриками (mAP, Precision, Recall), подтверждая результаты из отчета.

```bash
python src/evaluation/validate.py
```

---

## 3. Визуализация предсказаний на тестовых фото

Эта команда обработает изображения из тестовой выборки и сохранит их с нарисованными рамками.

```bash
python src/evaluation/predict.py
```

Результаты будут находиться в папке `runs/detect/predict/`.

---

## 4. Демонстрация на видео

Чтобы увидеть модель в действии на видео, выполните следующую команду, подставив имя одного из ваших видеофайлов из папки `data/raw`:

```bash
python src/evaluation/predict.py --source data/raw/1.mov (либо 2_1.mov , 3_1.mov , 3_2.mov , 4_1.mov , 4.mov)
```

Обработанное видео будет сохранено в новой папке внутри `runs/detect/`.

---

<details>
Полный цикл (для воспроизведения всех шагов с нуля)

Если вы хотите пройти весь путь от начала до конца, выполните следующие шаги после установки окружения:

1. Поместите видеофайлы (`.mov`) в папку `data/raw`.

2. Извлеките кадры:

    ```bash
    python src/data_preparation/extract_frames.py
    ```

3. Разметьте данные с помощью нашего инструмента, открыв в нем папку `data/extracted_frames`:

    ```bash
    python src/annotation/annotator.py
    ```

4. Разделите датасет на `train/val/test`:

    ```bash
    python src/data_preparation/split_dataset.py
    ```

5. Запустите обучение (рекомендуется HPO-версия):

    ```bash
    python src/training/train_hpo.py
    ```

</details>
